# -*- coding: utf-8 -*-
"""Final project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wzBQq-xY1B8GZxBBOUnn32OUQ0dql3zA

# Final Project Code
"""

# Commented out IPython magic to ensure Python compatibility.
#load libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import missingno
from mpl_toolkits.mplot3d import Axes3D
from sklearn.feature_extraction.text import CountVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import collections
from collections import Counter
import string
import re
from wordcloud import WordCloud
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk import bigrams
from pprint import pprint
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score, RepeatedStratifiedKFold
from sklearn.preprocessing import PowerTransformer, OneHotEncoder
from sklearn.compose import ColumnTransformer
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, GRU, Dropout
from tensorflow.keras.utils import to_categorical
from keras.callbacks import EarlyStopping, ModelCheckpoint

# View all columns and rows
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 90) # set to fit 90 column descriptions

# Set up notebook to display multiple outputs in one cell
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

tweets_df = pd.read_csv('/content/tweets.csv')

"""## EDA"""

# Inspect dataset

## Inspect columns and variable types
tweets_df.info()

## View dataframe rows
tweets_df.head()

## View summary statistics
tweets_df.describe()

## View missing values
tweets_df.isnull().sum()

## Plot missing values
missingno.matrix(tweets_df)

# Convert airline_sentiment classification to numeric: positive, neutral, negative = 1, 0, -1
tweets_df['numeric_sentiment'] = tweets_df.apply(lambda x: 1 if (x['airline_sentiment'] == 'positive') else 0 if (x['airline_sentiment'] == 'neutral') else -1, axis = 1)

# Create weighted_sentiment feature, the product of the sentiment and the confidence.
tweets_df['weighted_sentiment'] = tweets_df['numeric_sentiment'] * tweets_df['airline_sentiment_confidence']

# Compute summary statistics for numeric_sentiment variable.

print("Summary stats for numeric_sentiment.")
tweets_df['numeric_sentiment'].describe()

print("Median and mode for numeric_sentiment.")
tweets_df['numeric_sentiment'].median()
tweets_df['numeric_sentiment'].mode().iloc[0] # Taking first mode if multiple.

print("Summary stats for weighted_sentiment.")
tweets_df['weighted_sentiment'].describe()

print("Median and mode for weighted_sentiment.")
tweets_df['weighted_sentiment'].median()
tweets_df['weighted_sentiment'].mode().iloc[0] # Taking first mode if multiple.

## Noting that the 'numeric_sentiment' and 'weighted_sentiment' columns are predominantly negative.

# look at Airline Sentiment distribution
tweets_df.hist(column ='airline', by='numeric_sentiment',figsize=(12,8))
tweets_df.hist(column ='weighted_sentiment', by='airline',figsize=(12,8))
#sns.histplot(data_df,x = 'airline',hue = 'num_sentiment')
#plt.show()

# look at Negative Reason distribution
data_neg = tweets_df[tweets_df['numeric_sentiment'] < 0]
data_neg.hist(column ='weighted_sentiment', by='negativereason',figsize=(12,18))
data_neg.hist(column ='airline', by='negativereason',figsize=(12,18))

#create visuals

nltk.download('stopwords')
nltk.download('punkt')

# Check for missing values
print(tweets_df.isnull().sum())

# Explore sentiment distribution
sns.countplot(x='airline_sentiment', data=tweets_df, order=tweets_df['airline_sentiment'].value_counts().index)
plt.title('Sentiment Distribution')
plt.show()

# Explore sentiment distribution for each airline
sns.countplot(x='airline', hue='airline_sentiment', data=tweets_df)
plt.title('Sentiment Distribution by Airline')
plt.show()

# Word cloud for positive, negative, and neutral sentiments
company_names = ['Virgin America', 'United', 'Southwest', 'Delta', 'US Airways', 'American', 'JetBlue',
                'Southwest Air', 'SouthwestAir', 'VirginAmerica', 'AmericanAir', 'USAir', 'USAirways']
stop_words = set(stopwords.words('english') + company_names)

def plot_wordcloud(sentiment):
    words = ' '.join(tweets_df[tweets_df['airline_sentiment'] == sentiment]['text'])
    wordcloud = WordCloud(width=800, height=400, stopwords = stop_words, random_state=21, max_font_size=110).generate(words)
    plt.figure(figsize=(10, 7))
    plt.imshow(wordcloud, interpolation="bilinear")
    plt.axis('off')
    plt.title(f'Word Cloud for {sentiment} Sentiment')
    plt.show()

plot_wordcloud('positive')
plot_wordcloud('negative')
plot_wordcloud('neutral')

# Tokenize and analyze word frequencies

def process_text(text):
    words = word_tokenize(text)
    words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]
    return words

tweets_df['processed_text'] = tweets_df['text'].apply(process_text)

# Calculate and plot word frequencies
all_words = [word for sublist in tweets_df['processed_text'] for word in sublist]
freq_dist = FreqDist(all_words)
freq_dist.plot(30, cumulative=False)

"""## Cleaning the Data"""

# We note that the majority of airline_sentiment_gold is empty. However, non-null values may be validated (of a "gold standard").

diffs = tweets_df[tweets_df['airline_sentiment_gold'].notnull()]
diffs = (diffs['airline_sentiment'] != diffs['airline_sentiment_gold']).any()

print(diffs)

## There are no differences between airline_sentiment and airline_sentiment_gold. We remove the latter column.
tweets_df.drop(columns=['airline_sentiment_gold'], inplace=True)

# However, there are differences between those of negativereason and negativereason_gold.

# If confidence level in airline_sentiment_confidence is higher on average we'd suspect that negativereason_gold values are validated as compared with negativereason.
sentiment_mean = tweets_df.groupby('negativereason')['airline_sentiment_confidence'].mean()
gold_sent_mean = tweets_df.groupby('negativereason_gold')['airline_sentiment_confidence'].mean()

print(f"Mean of confidence for negativereason:\n{sentiment_mean}")
print(f"\nMean of confidence for negativereason_gold:\n{gold_sent_mean}")

## Since negativereason_gold has a higher mean confidence level in the value of the sentiment, we impute gold values over negativereason values.
tweets_df['imputed_neg_reason'] = tweets_df['negativereason_gold'].fillna(tweets_df['negativereason'])

check_df = tweets_df[(tweets_df['negativereason_gold'].notnull()) & (tweets_df['negativereason_gold'] != tweets_df['negativereason'])]
check_df = check_df[['negativereason', 'negativereason_gold', 'text']]
check_df

## A spot check confirms negativereason_gold offers more adequate information to the Tweet.

"""## Feature Engineering"""

# Cleaning user_timezone values in stages. tweet_location not as useful (e.g., one value is 'Earth' and that occurs in Eastern Time.)

# Clean timezone column
tweets_df['clean_timezone'] = tweets_df['user_timezone'].where(tweets_df['user_timezone'].str.contains('Time \('))

# Fill NaN values in the original column
tweets_df['user_timezone'].fillna('N/A', inplace=True)

# Unclean timezone column
tweets_df['unclean_timezone'] = tweets_df['user_timezone'].where(~tweets_df['clean_timezone'].notna())

## Code leaves in unclean_timezone column. We could delete the above line and simply use clean_timezone where timezone is known and clean.

# Convert 'tweet_created' column to datetime dtype
tweets_df['tweet_created'] = pd.to_datetime(tweets_df['tweet_created'])

# Extract day of the month
tweets_df['tweet_day'] = tweets_df['tweet_created'].dt.day

# Extract time of day
tweets_df['time_of_day'] = pd.cut(
    tweets_df['tweet_created'].dt.hour,
    bins=[0, 6, 12, 18, 24],
    labels=['Night', 'Morning', 'Afternoon', 'Evening'],
    include_lowest=True
)

"""### Pull Only Data Columns of Interest"""

#clean up data set
data_pre_process=tweets_df[['text','numeric_sentiment', 'weighted_sentiment','airline']]

"""### Initial Cleaning of Text Column and Prepare Data for Thorough Cleaning"""

#clean the text column for random forest modelling to identify key word counts
from bs4 import BeautifulSoup
import re
import nltk
# nltk.download()
from nltk.corpus import stopwords # Import the stop word list
nltk.download('stopwords')

data_size = (data_pre_process['text'].size)

"""### Cleaning the Data Using Beautiful Soup"""

#function to clean text usint Beautiful Soup
def clean_text_data(data_point, data_size):
    review_soup = BeautifulSoup(data_point)
    review_text = review_soup.get_text()
    review_letters_only = re.sub("[^a-zA-Z]", " ", review_text)
    review_lower_case = review_letters_only.lower()
    review_words = review_lower_case.split()
    stop_words = stopwords.words("english")
    meaningful_words = [x for x in review_words if x not in stop_words]

    if( (i)%2000 == 0 ):
        print("Cleaned %d of %d data (%d %%)." % ( i, data_size, ((i)/data_size)*100))

    return( " ".join( meaningful_words))

for i in range(data_size):
    data_pre_process["text"][i] = clean_text_data(data_pre_process["text"][i], data_size)
print("Cleaning training completed!")

"""### Executing a Random Forest Model Without Fitting to Learn Top Phrases and Number of Occurences for Each"""

vectorizer = CountVectorizer(analyzer = "word",   \
                             ngram_range= (2,3),   \
                             tokenizer = None,    \
                             preprocessor = None, \
                             stop_words = ['united','usairways','americanair','southwestair','jetblue','http','http co','co','virginamerica'])


X_train, X_cv, Y_train, Y_cv = train_test_split(data_pre_process["text"], data_pre_process["numeric_sentiment"], test_size = 0.2, random_state=42)

X_train = vectorizer.fit_transform(X_train)
X_train = X_train.toarray()
print(X_train.shape)

X_cv = vectorizer.transform(X_cv)
X_cv = X_cv.toarray()
print(X_cv.shape)

vocab = vectorizer.get_feature_names_out()
distribution = np.sum(X_train, axis=0)

#for tag, count in zip(distribution[:100],vocab[:100]):
#    print(count, tag)

zipped = zip (distribution,vocab)
zipped = list(zipped)
zipped.sort(reverse=True)
zip_top = zipped[:10]
zip_top_df = pd.DataFrame(zip_top)
#zip_top_df.head()
graph = sns.barplot(zip_top_df,x=zip_top_df[1],y=zip_top_df[0])
graph.set_xticklabels(
    labels=zip_top_df[1], rotation=-30)
plt.show()

"""## Bag of Words"""

tweets_df1 = tweets_df.copy()
tweets_df1['text'] = tweets_df1['text'].astype(str)

stop_words = set(stopwords.words('english'))
stop_words.remove('not') #need 'not' as a negative identifier in tweets

string.punctuation

#clean Tweet text
ps = PorterStemmer()

tweet_list = []

for i in range(0, 14640):
    tweet = re.sub('@[^\s]+','', tweets_df1['text'][i])
    tweet = re.sub('http[^\s]+','', tweet)
    tweet = re.sub('['+string.punctuation+']', '', tweet)
    tweet = tweet.lower()
    tweet = tweet.split()
    tweet = [ps.stem(word) for word in tweet if not word in stop_words]
    tweet = ' '.join(tweet)
    tweet_list.append(tweet)

#remove emojis
def remove_emoji(text):
    emojis = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)

    text = re.sub(emojis,'',text)
    return text

tweet_list = list(map(remove_emoji, tweet_list))

len(tweet_list)
tweet_list[1:5]

#find unique words for count vectorizor
text = ' '.join(tweet_list)
words = text.split()
unique_words = set(words)
print(len(unique_words))

#look at word counts
word_count = Counter(words)
sorted_word_count = sorted(word_count.items(), key = lambda x: x[1], reverse=True)
for word, count in sorted_word_count:
    print(f'{word}: {count}')

#find all words that occur at least 5 times
count_mult_words = sum(1 for word, count in sorted_word_count if count >= 5)
print(count_mult_words)

"""### Count Vectorizer"""

cv = CountVectorizer(max_features = 2471)
cv.fit(tweet_list)

#create bag of words
bow = cv.transform(tweet_list)
bow.shape

#create bag of words dataframe
bow_df = pd.DataFrame(bow.toarray())
bow_df.columns = cv.get_feature_names_out()
bow_df.head()

"""## Join Dataframes"""

tweets_df2 = pd.concat([tweets_df1, bow_df.set_axis(tweets_df1.index)], axis=1)
tweets_df2.info()
tweets_df2.head()

"""### Drop Irrelevant Columns"""

tweets_df3 = tweets_df2.copy()

columns = ['airline_sentiment', 'airline_sentiment_confidence', 'negativereason',
           'negativereason_confidence', 'name', 'negativereason_gold', 'text', 'tweet_coord',
           'tweet_created', 'tweet_location', 'user_timezone', 'weighted_sentiment', 'processed_text',
          'unclean_timezone']

tweets_df3 = tweets_df3.drop(columns, axis = 1)
tweets_df3.head()

"""## Encoding"""

#encode categorical variables

# Converting type of columns to category
tweets_df3['airline'] = tweets_df3['airline'].astype('category')
tweets_df3['imputed_neg_reason'] = tweets_df3['imputed_neg_reason'].astype('category')
tweets_df3['clean_timezone'] = tweets_df3['clean_timezone'].astype('category')
tweets_df3['time_of_day'] = tweets_df3['time_of_day'].astype('category')

# Assigning numerical values and storing it in another columns
tweets_df3['airline_new'] = tweets_df3['airline'].cat.codes
tweets_df3['imputed_neg_reason_new'] = tweets_df3['imputed_neg_reason'].cat.codes
tweets_df3['clean_timezone_new'] = tweets_df3['clean_timezone'].cat.codes
tweets_df3['time_of_day_new'] = tweets_df3['time_of_day'].cat.codes

# one hot encoder
enc = OneHotEncoder()

# Passing encoded columns
enc_data = pd.DataFrame(enc.fit_transform(
    tweets_df3[['airline_new', 'imputed_neg_reason_new', 'clean_timezone_new', 'time_of_day_new']]).toarray())

# Merge with main
tweets_df4 = tweets_df3.join(enc_data)
tweets_df4.columns = tweets_df4.columns.astype(str)
tweets_df4 = tweets_df4.drop(['airline', 'imputed_neg_reason', 'clean_timezone', 'time_of_day'], axis = 1)

tweets_df4.head()
tweets_df4.info()

"""## Create Models"""

#split train and test datasets 80/20 split
train_df, rest_df = train_test_split(tweets_df4, train_size=0.8, shuffle=False)

#split validation and test data 10/10 split
val_df, test_df = train_test_split(rest_df, test_size = 0.5, shuffle = False)

#get targets
train_target = train_df['numeric_sentiment']
train_tweet_ids = train_df['tweet_id']
train_df = train_df.drop(['numeric_sentiment', 'tweet_id'], axis = 1)

val_target = val_df['numeric_sentiment']
val_tweet_ids = val_df['tweet_id']
val_df = val_df.drop(['numeric_sentiment', 'tweet_id'], axis = 1)

test_target = test_df['numeric_sentiment']
test_tweet_ids = test_df['tweet_id']
test_df = test_df.drop(['numeric_sentiment', 'tweet_id'], axis = 1)

train_df.head()

"""### Naive Bayes"""

X = train_df
y = train_target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12)

nb = GaussianNB()
print('Current Parameters:\n')
pprint(nb.get_params())

classifier = GaussianNB()
classifier.fit(X_train, y_train)

cv_scores = cross_val_score(classifier, X, y, cv=5)

print(classifier, ' mean accuracy: ', round(cv_scores.mean()*100, 3), '% std: ', round(cv_scores.var()*100, 3),'%')

"""##### Tune Hyperparameters"""

np.logspace(0,-9, num=10)

cv_method = RepeatedStratifiedKFold(n_splits=5,
                                    n_repeats=3,
                                    random_state=999)

params_nb = {'var_smoothing': np.logspace(0,-9, num = 100)}

gcv_nb = GridSearchCV(estimator = classifier,
                     param_grid = params_nb,
                     cv = cv_method,
                     verbose = 1,
                     scoring = 'accuracy')

df_transformed = PowerTransformer().fit_transform(X_test)

gcv_nb.fit(df_transformed, y_test)

model_nb = gcv_nb

gcv_nb.best_score_

gcv_nb.best_params_

"""##### Fit Model"""

y_pred = model_nb.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)

accuracy_score(y_test, y_pred)

"""##### Fit Test Data"""

nb_pred = model_nb.predict(test_df)

cm = confusion_matrix(test_target, nb_pred)
print(cm)

accuracy_score(test_target, nb_pred)

"""### Random Forest Classifier"""

rf = RandomForestClassifier(random_state = 12)
print('Current Parameters:\n')
pprint(nb.get_params())

classifier = RandomForestClassifier(n_estimators = 400, criterion = 'entropy', random_state = 12)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)

accuracy_score(y_test, y_pred)

print(classification_report(y_pred, y_test))

"""##### Tune Hyperparameters"""

param_grid = {
    'n_estimators': [25, 50, 100, 150],
    'max_features': ['sqrt', 'log2', None],
    'max_depth': [3, 6, 9],
    'max_leaf_nodes': [3, 6, 9],
}


grid_search = GridSearchCV(RandomForestClassifier(),
                           param_grid=param_grid)

grid_search.fit(X_train, y_train)
print(grid_search.best_estimator_)

"""##### Fit Model"""

model_rf = RandomForestClassifier(max_depth = 9,
                                 max_features = None,
                                 max_leaf_nodes = 9,
                                 n_estimators = 25)

model_rf.fit(X_train, y_train)
y_pred_rf = model_rf.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)

accuracy_score(y_test, y_pred)

print(classification_report(y_pred, y_test))

"""##### Fit Test Data"""

rf_pred = model_rf.predict(test_df)

cm = confusion_matrix(test_target, rf_pred)
print(cm)

accuracy_score(test_target, rf_pred)

"""### LSTM Model"""

tweets_df5 = tweets_df1.copy()
tweets_df5['clean_text'] = pd.Series(tweet_list)
tweets_df5.head()

"""##### Tokenize"""

max_fatures = 2471
tokenizer = Tokenizer(num_words=max_fatures, split=' ')
tokenizer.fit_on_texts(tweets_df5['clean_text'].values)
X = tokenizer.texts_to_sequences(tweets_df5['clean_text'].values)
X = pad_sequences(X)

"""##### Build Model"""

embed_dim = 128
lstm_out = 196

model_lstm = Sequential()
model_lstm.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))
model_lstm.add(SpatialDropout1D(0.2))
model_lstm.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))
model_lstm.add(Dense(3,activation='softmax'))
model_lstm.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])
print(model_lstm.summary())

Y = pd.get_dummies(tweets_df5['numeric_sentiment']).values
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

"""##### Fit Model"""

batch_size = 32
model_lstm.fit(X_train, Y_train, epochs = 15, batch_size = batch_size, verbose = 2)

validation_size = 1500

X_validate = X_test[-validation_size:]
Y_validate = Y_test[-validation_size:]
X_test = X_test[:-validation_size]
Y_test = Y_test[:-validation_size]
score,acc = model_lstm.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)
print("score: %.2f" % (score))
print("acc: %.2f" % (acc))

"""### GRU Model

##### Tokenize
"""

# Preprocessing
X = tweets_df5["clean_text"]
y = pd.get_dummies(tweets_df5['numeric_sentiment']).values

# Tokenization
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X_seq = tokenizer.texts_to_sequences(X)
vocab_size = len(tokenizer.word_index) + 1

# Padding sequences
max_length = max([len(seq) for seq in X_seq])
X_pad = pad_sequences(X_seq, maxlen=max_length, padding='post')

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size = 0.1, random_state=42)

"""##### Build Model"""

# Build GRU model
model_gru = Sequential()
model_gru.add(Embedding(vocab_size, 100, input_length=max_length))
model_gru.add(GRU(64))
model_gru.add(Dropout(0.5))
model_gru.add(Dense(3, activation='softmax'))
model_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
print(model_gru.summary())

"""##### Fit Model"""

batch_size = 64
model_gru.fit(X_train, y_train, epochs=15, batch_size=batch_size, validation_data=(X_test, y_test))

k_fold = KFold(n_splits=5, shuffle=True, random_state=42)

# Initialize lists to store evaluation metrics
scores = []

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model_cv.h5', monitor='val_loss', save_best_only=True)

# Perform K-fold cross-validation
for fold_index, (train_indices, val_indices) in enumerate(k_fold.split(X_pad, y)):
    print(f"Fold {fold_index + 1}/{k_fold.n_splits}")

    # Split data into training and validation sets for this fold
    X_train_fold, X_val_fold = X_pad[train_indices], X_pad[val_indices]
    y_train_fold, y_val_fold = y[train_indices], y[val_indices]

    # Build GRU model
    model_gru_cv = Sequential()
    model_gru_cv.add(Embedding(vocab_size, 100, input_length=max_length))
    model_gru_cv.add(GRU(64))
    model_gru_cv.add(Dropout(0.5))
    model_gru_cv.add(Dense(3, activation='softmax'))
    model_gru_cv.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # Train the model with callbacks
    history = model_gru_cv.fit(X_train_fold, y_train_fold, epochs=15, batch_size=batch_size,
                               validation_data=(X_val_fold, y_val_fold),
                               callbacks=[early_stopping, model_checkpoint],
                               verbose=1)

    # Evaluate the model on the validation set
    score = model_gru_cv.evaluate(X_val_fold, y_val_fold, verbose=0)
    print("Validation Score:", score)
    scores.append(score)

# Calculate mean and standard deviation of validation scores
mean_score = np.mean(scores, axis=0)
std_score = np.std(scores, axis=0)
print("Mean Validation Score:", mean_score)
print("Standard Deviation of Validation Score:", std_score)

history_final = model_gru.fit(X_pad, y, epochs=15, batch_size=batch_size, verbose=1)

# Predict probabilities for each class on the entire dataset
probs = model_gru.predict(X_pad, verbose=2, batch_size=batch_size)

# Determine predicted classes based on the highest probability
y_pred = np.argmax(probs, axis=1)

# Convert one-hot encoded labels to single label for the entire dataset
y_true = np.argmax(y, axis=1)

# Generate classification report for the entire dataset
print("Classification Report:")
print(classification_report(y_true, y_pred))